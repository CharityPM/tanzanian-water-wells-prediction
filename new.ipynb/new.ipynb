{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 **TANZANIAN WATER WELLS PREDICTION**\n",
    "\n",
    "## **1.Business understanding Problem and Objective**\n",
    "\n",
    "### Stateholder:Tanzanian NGO\n",
    "\n",
    "### **Overview**\n",
    "\n",
    "#### Tanzania faces significant challenges in providing consistent access to clean water for its citizens. With an estimated population of 70.5 million in mid‑2025, Tanzania remains one of Africa’s fastest-growing nations in terms of population and resource demand.\n",
    "\n",
    "### **Business problem**\n",
    "#### The client seeks insights into patterns distinguishing **functional** versus **non-functional or repair-needed wells**, to inform drilling strategies and maintenance priorities. The goal is to help villages avoid prolonged downtime and reduce socio-economic burdens especially on women and children, who are often tasked with collecting water.\n",
    "\n",
    "### **Project objective**\n",
    "#### Use the Pump It Up dataset (featuring features like pump type, installation year, payment method, and location) to build a **classification model** that:\n",
    "#### - Predict well status in a **binary setup**: functional vs. needs attention (repair or non-functional).  \n",
    "#### -  Identify the key features driving pump failures (e.g., pump type, installation year, management system).\n",
    "\n",
    "### **Project goal/value**\n",
    "#### Enables targeted maintenance, directing resources to at-risk wells before they fail.#### Reduces the socioeconomic burden of water insecurity by ensuring greaterhealth, education, and gender equity outcomes.\n",
    "\n",
    "### **Data source**\n",
    "#### DrivenData. (2015). Pump it Up: Data Mining the Water Table. Retrieved [Month Day Year] from https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table.\n",
    "\n",
    "\n",
    "\n",
    "###  2.Data loading and overview\n",
    "#### 2.1.import libraries\n",
    "#### These libraries will support data exploration, preprocessing, modeling, and evaluation:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from scipy.stats import randint\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2 Load dataset\n",
    "# train dataset\n",
    "df_labels=pd.read_csv(\"Dataset/Training set labels.csv\")\n",
    "df_values=pd.read_csv(\"Dataset/Training set values.csv\")\n",
    "\n",
    "print(\"Labels:\")\n",
    "display(df_labels.head())\n",
    "\n",
    "print(\"\\nValues:\")\n",
    "display(df_values.head())\n",
    "#merge the two datasets using id\n",
    "train_df = df_values.merge(df_labels, on='id')\n",
    "display(train_df.head())\n",
    "print(\"Combined shape:\", train_df.shape)\n",
    "\n",
    "#load test set\n",
    "test_df = pd.read_csv('Dataset/Test set values.csv')\n",
    "\n",
    "\n",
    "#combining train and test data for cleaning and preprocessing\n",
    "#Add a flag so you can split them later\n",
    "train_df['source'] = 'train'\n",
    "test_df['source'] = 'test'\n",
    "test_df['status_group'] = np.nan  # placeholder for label column\n",
    "\n",
    "# Combine\n",
    "df= pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "#checking columns in our data\n",
    "df.columns\n",
    "\n",
    "#get data type of columns and non-null count\n",
    "df.info()\n",
    "def data_types(data):\n",
    "\n",
    "    print(\"Our dataset has\", len( data.select_dtypes(include='number').columns),\n",
    "                \"numeric columns\")\n",
    "    \n",
    "    print(\"and\", len(data.select_dtypes(include='object').columns),\n",
    "          \"categorical columns\")\n",
    "data_types(df) \n",
    "### 3.1 Data cleaning and Preprocessing\n",
    "\n",
    "#### a) Duplicates\n",
    "#checking duplicates using a function\n",
    "duplicates = []\n",
    "\n",
    "def check_duplicates(data):\n",
    "\n",
    "    \"\"\"Function that iterates through the rows of our dataset to check whether they are duplicated or not\"\"\"\n",
    "    \n",
    "    for i in data.duplicated():\n",
    "        duplicates.append(i)\n",
    "    duplicates_values = set(duplicates)\n",
    "    if(len(duplicates_values) == 1):\n",
    "        print('Our Dataset has no Duplicates')\n",
    "\n",
    "    else:\n",
    "        duplicates_percentage = np.round(((sum(duplicates)/len(data)) * 100 ), 2)\n",
    "        print(f'Duplicated rows constitute of {duplicates_percentage} % of our dataset')\n",
    "        \n",
    "\n",
    "check_duplicates(df)\n",
    "#### b) Missing values\n",
    "#checking missing values\n",
    "# Get total and percentage of missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n",
    "missing_df = missing_df[missing_df['Missing Values'] > 0].sort_values(by='Percentage', ascending=False)\n",
    "\n",
    "print(\"Missing values in dataset:\")\n",
    "display(missing_df)\n",
    "\n",
    "\n",
    "### Scheme_name and Scheme Management\n",
    "#### The main objective of this project is to analyze data in order to predict whether water wells in Tanzania are functional or non-functional. The features scheme_name and scheme_management provide information about the names of water supply schemes and their managing entities.However, these features are not expected to be strong predictors of well functionality.\n",
    "#### In addition, both columns have approximately 47% missing values, which is nearly half of the dataset. Imputing these missing values would not only be difficult but also unlikely to add significant predictive power to the model.\n",
    "#### Therefore we will drop them.\n",
    "#dropping scheme name & scheme management.\n",
    "df.drop(['scheme_name','scheme_management'], axis=1,inplace=True)\n",
    "#preview shape after dropping\n",
    "df.shape\n",
    "### Funder\n",
    "#### The funder feature contains the names of organizations or individuals who contributed to the installation of the wells, which could offer valuable insights for future partnerships and resource allocation.\n",
    "#### However, this column has approximately 6% missing values. Since the proportion is relatively small and the information could be meaningful, we will drop only the rows with missing values in this column and select only top 20 funders.\n",
    "#1. Check value counts for funder\n",
    "funder_counts = df['funder'].value_counts()\n",
    "funder_counts\n",
    "\n",
    "#Keep only the top 20 funders\n",
    "top_n = 20\n",
    "top_funders = funder_counts.nlargest(top_n).index\n",
    "\n",
    "df['funder_reduced'] = df['funder'].where(df['funder'].isin(top_funders), 'Other')\n",
    "df = df.drop(columns=['funder'])\n",
    "\n",
    "\n",
    "df.shape\n",
    "### Installer\n",
    "#### This refers to contractors who are installing the wells.This would contribute much to our stakeholder on the best contractors to work with.\n",
    "#### The  percentage of the missing values are not much ,we will therefore drop the rows with the missing values only.\n",
    "\n",
    "##### The installer column also contains names of organizations or entities responsible for installing water wells. However, this column includes many inconsistencies due to:\n",
    "\n",
    "- **Spelling errors** (e.g., `Distric Water Department` vs. `District Water Department`)\n",
    "- **Variations in formatting** (e.g., `GOVERNMENT`, `Government`, `Governmen`)\n",
    "- **Synonyms or partial names** representing the same entity\n",
    "\n",
    "##### To clean this, we used a **dictionary mapping** approach. This method replaces all known variations and misspellings with a consistent, unified label.\n",
    "# Dictionary of common replacements for installer names\n",
    "installer_mapping = {\n",
    "    # District Council\n",
    "    'District Water Department': 'District water department',\n",
    "    'District water depar': 'District water department',\n",
    "    'Distric Water Department': 'District water department',\n",
    "    \n",
    "    # Fini Water\n",
    "    'FinW': 'Fini Water',\n",
    "    'Fini water': 'Fini Water',\n",
    "    'FINI WATER': 'Fini Water',\n",
    "    \n",
    "    # Jaica\n",
    "    'JAICA': 'Jaica',\n",
    "    \n",
    "    # District Council Variants\n",
    "    'COUN': 'District council',\n",
    "    'District COUNCIL': 'District council',\n",
    "    'DISTRICT COUNCIL': 'District council',\n",
    "    'District Counci': 'District council',\n",
    "    'District Council': 'District council',\n",
    "    'Council': 'District council',\n",
    "    'Counc': 'District council',\n",
    "    'District  Council': 'District council',\n",
    "    'Distri': 'District council',\n",
    "    \n",
    "    # RC Church Variants\n",
    "    'RC CHURCH': 'RC Church',\n",
    "    'RC Churc': 'RC Church',\n",
    "    'RC': 'RC Church',\n",
    "    'RC Ch': 'RC Church',\n",
    "    'RC C': 'RC Church',\n",
    "    'RC CH': 'RC Church',\n",
    "    'RC church': 'RC Church',\n",
    "    'RC CATHORIC': 'RC Church',\n",
    "    \n",
    "    # Central Government Variants\n",
    "    'Central Government': 'Central government',\n",
    "    'Tanzania Government': 'Central government',\n",
    "    'central government': 'Central government',\n",
    "    'Cental Government': 'Central government',\n",
    "    'Cebtral Government': 'Central government',\n",
    "    'Tanzanian Government': 'Central government',\n",
    "    'Tanzania government': 'Central government',\n",
    "    'Centra Government': 'Central government',\n",
    "    'CENTRAL GOVERNMENT': 'Central government',\n",
    "    'TANZANIAN GOVERNMENT': 'Central government',\n",
    "    'Central govt': 'Central government',\n",
    "    'Centr': 'Central government',\n",
    "    'Centra govt': 'Central government',\n",
    "    \n",
    "    # World Vision\n",
    "    'World vision': 'World Vision',\n",
    "    'World Division': 'World Vision',\n",
    "    \n",
    "    # Unicef\n",
    "    'Unisef': 'Unicef',\n",
    "    'UNICEF': 'Unicef',\n",
    "    \n",
    "    # DANIDA\n",
    "    'DANID': 'DANIDA',\n",
    "    \n",
    "    # Villagers\n",
    "    'villigers': 'villagers',\n",
    "    'villager': 'villagers',\n",
    "    'Villagers': 'villagers',\n",
    "    'Villa': 'villagers',\n",
    "    'Village': 'villagers',\n",
    "    'Villi': 'villagers',\n",
    "    'Village Council': 'villagers',\n",
    "    'Village Counil': 'villagers',\n",
    "    'Villages': 'villagers',\n",
    "    'Vill': 'villagers',\n",
    "    'Village community': 'villagers',\n",
    "    'Villaers': 'villagers',\n",
    "    'Village Community': 'villagers',\n",
    "    'Villag': 'villagers',\n",
    "    'Villege Council': 'villagers',\n",
    "    'Village council': 'villagers',\n",
    "    'Village  Council': 'villagers',\n",
    "    'Villagerd': 'villagers',\n",
    "    'Villager': 'villagers',\n",
    "    'Village Technician': 'villagers',\n",
    "    'Village Office': 'villagers',\n",
    "    'Village community members': 'villagers',\n",
    "    \n",
    "    # Community\n",
    "    'Commu': 'Community',\n",
    "    'Communit': 'Community',\n",
    "    'commu': 'Community',\n",
    "    'COMMU': 'Community',\n",
    "    'COMMUNITY': 'Community',\n",
    "    \n",
    "    # Government Variants\n",
    "    'GOVERNMENT': 'Government',\n",
    "    'GOVER': 'Government',\n",
    "    'GOVERNME': 'Government',\n",
    "    'GOVERM': 'Government',\n",
    "    'GOVERN': 'Government',\n",
    "    'Gover': 'Government',\n",
    "    'Gove': 'Government',\n",
    "    'Governme': 'Government',\n",
    "    'Governmen': 'Government',\n",
    "    \n",
    "    # Other custom fixes\n",
    "    'Hesawa': 'HESAWA',\n",
    "    'Colonial Government': 'Colonial government',\n",
    "    'Government of Misri': 'Misri Government',\n",
    "    'Italy government': 'Italian government',\n",
    "    'British colonial government': 'British government',\n",
    "    'Concern /government': 'Concern/Government',\n",
    "    'Village Government': 'Village government',\n",
    "    'Government and Community': 'Government /Community',\n",
    "    'Cetral government /RC': 'RC church/Central Gover',\n",
    "    'Government /TCRS': 'TCRS /Government',\n",
    "    'Government/TCRS': 'TCRS /Government',\n",
    "    'ADRA /Government': 'ADRA/Government'\n",
    "}\n",
    "\n",
    "# Apply the replacements\n",
    "df['installer'] = df['installer'].replace(installer_mapping)\n",
    "\n",
    "### Public Meeting\n",
    "#### Public meetings are a way of the community to come together and raise issues of concern. We will also just drop missing values of the column.\n",
    "#drop missing values in funder,installer,public meeting\n",
    "df = df.dropna(subset=['funder_reduced', 'installer', 'public_meeting'])\n",
    "#preview shape\n",
    "df.shape\n",
    "## 4.Exploratory Data Analysis (EDA)\n",
    "### 4.1 Binary Target Variable Creation and Class Distribution\n",
    "#### To simplify the prediction task, have converted the original status_group target variable into a binary classification problem as per the objectives. Wells labeled as \"functional\" remain as is, while those labeled as \"needs_repair\" or \"non-functional\" were combined into a single \"needs_attention\" class.\n",
    "# Create binary target variable\n",
    "df['status_binary'] = df['status_group'].apply(lambda x: 'functional' if x == 'functional' else 'needs_attention')\n",
    "\n",
    "\n",
    "\n",
    "#### Class Distribution\n",
    "# Count how many rows in each class\n",
    "class_counts = df['status_binary'].value_counts()\n",
    "print(\"Number of wells per class:\")\n",
    "print(class_counts)\n",
    "\n",
    "# Display the proportion (percentage) per class\n",
    "class_proportions = df['status_binary'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage distribution per class:\")\n",
    "print(class_proportions.round(2))\n",
    "\n",
    "# Plot the class distribution\n",
    "class_counts.plot(kind='bar', color=['green', 'red'], title='Binary Target Class Distribution')\n",
    "plt.xlabel('Well Status')\n",
    "plt.ylabel('Number of Wells')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "#### The dataset is fairly balanced with respect to the binary target classes:This shows that:\n",
    "#### About 56.44% of wells are functional\n",
    "#### About 43.56% need attention\n",
    "#### This near-even split ensures that our model will have sufficient examples from both classes to learn meaningful patterns, reducing the risk of bias towards the majority class.\n",
    "### 4.2 **Univariate Analysis (Categorical Features)**\n",
    "#### Exploring key categorical variables to understand their distributions.\n",
    "# visualizing Catogorical variables\n",
    "# Drop ID column\n",
    "if 'id' in df.columns:\n",
    "    df = df.drop(columns=['id'])\n",
    "\n",
    "# Categorical columns to visualize\n",
    "categorical_cols = ['funder_reduced', 'installer', 'payment_type', 'water_quality', 'source_type']\n",
    "\n",
    "# Setup the subplot grid\n",
    "num_plots = len(categorical_cols)\n",
    "cols = 2  \n",
    "rows = (num_plots + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(14, 5 * rows))\n",
    "axes = axes.flatten()  # Flatten to 1D array for easy indexing\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    top10 = df[col].value_counts().iloc[:10].index\n",
    "    sns.countplot(data=df, x=col, order=top10, ax=axes[i], palette='Set2')\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "    axes[i].set_xlabel(\"\")\n",
    "    axes[i].tick_params(axis='x', rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#### From the visualization of the categorical columns,it is clear that:\n",
    "#### - **Funder:** Government of Tanzania funds the majority of wells.\n",
    "#### - **Installer:** DWE is the most frequent installer — their wells' performance may be significant.\n",
    "#### - **Payment Type:** Most wells are community-funded (free), suggesting non-commercial purpose.\n",
    "#### - **Water Quality:** Majority use soft water relevant for assessing water safety.\n",
    "#### - **Source Type:** Shallow wells, springs, and boreholes dominate, potentially influencing functionality.\n",
    "\n",
    "### 4.3 Distribution of Numerical values\n",
    "###  Checking outliers \n",
    "#checking outliers in numerical data using Interquartile range\n",
    "#select numerical columns only\n",
    "numerical_data=df.select_dtypes(include='number')\n",
    "\n",
    "\n",
    "for col in numerical_data.columns:\n",
    "    Q1 = numerical_data[col].quantile(0.25)\n",
    "    Q3 = numerical_data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Calculate bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Extract outlier rows\n",
    "    outliers = df[(numerical_data[col] < lower_bound) | (numerical_data[col] > upper_bound)]\n",
    "\n",
    "    print(f\"Column: {col}\")\n",
    "    if not outliers.empty:\n",
    "        print(outliers[[col]])\n",
    "    else:\n",
    "        print(\"No outliers found.\")\n",
    "    print(\"-\" * 50)\n",
    "#Visualizating the outliers in our numerical data\n",
    "def plt_boxplots(data,cols):\n",
    "    fig, axes =plt.subplots(2,4, figsize=(20,10))\n",
    "    axes=axes.ravel()\n",
    "    \n",
    "\n",
    "    colors=['black','skyblue','green','blue','black','purple','pink','brown','gray']\n",
    "\n",
    "    for i,col in enumerate(cols[:8]):\n",
    "        # convert the x-axis variable to a numeric data type\n",
    "        \n",
    "        data[col] = data[col].astype(float)\n",
    "        sns.boxplot(x=data[col], ax=axes[i], color=colors[i])\n",
    "        \n",
    "        axes[i].set_title(col)\n",
    "    plt.tight_layout()\n",
    "cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "plt_boxplots(df, cols)\n",
    "\n",
    "\n",
    "##### **Amount_tsh**-Highly skewed with many extreme outliers on the right.Most of the data concentrated near zero.outliers to be handled or use log transformation.\n",
    "##### **gps height**-More symmetric distribution, but still shows some right-side outliers.\n",
    "##### **construction year**-Relatively well-distributed with no strong outliers.Appears normally distributed with a peak near a central value.\n",
    "##### **Population**-Very right-skewed — many outliers in high population areas.Most wells serve a small population, with a few outliers serving large communities.\n",
    "##### **num_plate**-Highly skewed and sparse — almost all values are 0 or low.Presence of extreme outliers might be a low-importance feature.\n",
    "##### **Region code** and **district code**-Both appear more categorical than numeric.to be ploted as barplot.\n",
    "\n",
    "\n",
    "#### 4.3b Irrelevant columns\n",
    "##### The following columns appear not to have relevant infomation towards our project,we will therefore  drop them:(**Permit,Subvillage,wpt_name,region_code,district_code,iga, ward,recorded_by and date_recorded**)\n",
    "\n",
    "irrelevant_columns = ['permit', 'subvillage', 'wpt_name', 'region_code', 'district_code',\n",
    "                      'lga', 'ward', 'recorded_by', 'date_recorded','num_private']\n",
    "\n",
    "# Keep only those columns that are actually still in the DataFrame\n",
    "columns_to_drop = [col for col in irrelevant_columns if col in df.columns]\n",
    "\n",
    "# Drop the invalid columns\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "print(f\"Dropped columns: {columns_to_drop}\")\n",
    "print(f\"Current shape: {df.shape}\")\n",
    "### 4.4 Relationship between Target variable and features\n",
    "#### Categorical vs Target variable\n",
    "\n",
    "#Layout\n",
    "n_cols =2\n",
    "n_rows=(len(categorical_cols)+n_cols-1) // n_cols\n",
    "\n",
    "# create figure\n",
    "fig,axes=plt.subplots(n_rows, n_cols,figsize=(16,5 * n_rows))\n",
    "axes=axes.flatten()\n",
    "\n",
    "for i,col in enumerate(categorical_cols):\n",
    "    top10 = df[col].value_counts().iloc[:10].index\n",
    "    sns.countplot(data=df, x=col, hue='status_binary', order=top10, ax=axes[i], palette='Set2')\n",
    "    axes[i].set_title(f'{col} vs Well Status')\n",
    "    axes[i].tick_params(axis='x', rotation=90)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#### From the visualization ,relationship between categorical vs target is as follows:\n",
    "#### **Funder vs status** -Government of Tanzania funds the largest number of wells, but a higher portion of these wells need attention than are functional. Danida, Rwssp, and World Vision show a more balanced distribution, with relatively more functional wellsHesawa, Kkkt, and World Bank have a higher number of wells needing attention.\n",
    "#### **Installer vs status**-DWE is by far the most common installer, with a large number of wells both functional and needing attention.\n",
    "#### **Payment vs status**-\"Never pay\" is the most common and has high failure.\"Per bucket\" and \"monthly\" payments align more with functional wells, possibly reflecting better maintenance via community investment.\n",
    "### **water Quality vs Status**-Wells with soft water are more likely to be functional.wells with salty or milky water show higher need for attention.\n",
    "#### **Source type vs status** -Spring and river/lake sources show better functionality.Shallow wells and boreholes are more prone to needing attention.\n",
    "\n",
    "\n",
    "### 4.5 Numerical vs Target variable\n",
    "# Select only key numeric features for comparison\n",
    "selected_numeric_cols = ['amount_tsh', 'gps_height', 'population', 'construction_year']\n",
    "# layout\n",
    "num_features = len(selected_numeric_cols)\n",
    "cols_per_row = 3\n",
    "rows = (num_features + cols_per_row - 1) // cols_per_row\n",
    "\n",
    "plt.figure(figsize=(18, 5 * rows))\n",
    "\n",
    "for i, col in enumerate(selected_numeric_cols, 1):\n",
    "    plt.subplot(rows, cols_per_row, i)\n",
    "    sns.boxplot(data=df, x='status_binary', y=col, palette='Set2')\n",
    "    plt.title(f'{col} vs Well Status')\n",
    "    plt.xlabel('Well Status')\n",
    "    plt.ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#### **\n",
    "### 5.Preprocessing\n",
    "\n",
    "### 5.1 Log transformation for skewed numerical features\n",
    "#### amount_tsh and population are highly skewed.applying log transformation will compress the scale for a better model.\n",
    "# Log transform skewed features\n",
    "df['amount_tsh_log'] = np.log1p(df['amount_tsh'])\n",
    "df['population_log'] = np.log1p(df['population'])\n",
    "\n",
    "#Drop original amount_tsh and population\n",
    "df.drop(['amount_tsh', 'population'], axis=1, inplace=True)\n",
    "\n",
    "### 5.3 Construction year to Well Age\n",
    "##### The well_age feature represents the age of a water well calculated as the difference between the current year (2025) and the year the well was constructed. This helps us understand how the age of a well affects its functionality, as older wells might be more prone to failures.\n",
    "##### Since some wells have invalid construction years (e.g. 0), we replace these with missing values and then impute using the median well age to maintain data integrity.\n",
    "# Calculate well age\n",
    "df['well_age'] = 2025 - df['construction_year']\n",
    "\n",
    "# Handle invalid years (e.g., 0)\n",
    "df.loc[df['well_age'] == 2025, 'well_age'] = np.nan  # Replace invalid '0' year wells with NaN\n",
    "\n",
    "# Impute missing well_age with median value\n",
    "df['well_age'] = df['well_age'].fillna(df['well_age'].median())\n",
    "### 5.2 Encode Categorical Variables\n",
    "##### Machine learning model will require numerical inputs.We will use one-hot encoding to convert categorical features into binary.To avoid dummy variable trap, we will drop the first category from each one-hot encoded feature using drop_first=True\n",
    "#identify our cleaned categorical columns exclude the target.\n",
    "categ_cols=cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "categ_cols = [col for col in categ_cols if col not in ['status_group', 'status_binary']]\n",
    "print(\"Initial categorical columns:\")\n",
    "print(categ_cols)\n",
    "\n",
    "# Encode selected categorical variables using One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df,categ_cols, columns=categ_cols, drop_first=True)\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "### 5.3 Split dataset back to Train and Test\n",
    "# Separate train and test based on the 'source' column\n",
    "train_cleaned = df_encoded[df_encoded['source_train'] == 1].copy()\n",
    "test_cleaned = df_encoded[df_encoded['source_train'] == 0].copy()\n",
    "\n",
    "# Drop the source columns\n",
    "train_cleaned.drop(columns=['source_train'], inplace=True)\n",
    "test_cleaned.drop(columns=['source_train', 'status_group', 'status_binary'], errors='ignore')\n",
    "train_cleaned.columns\n",
    "### 6. Modeling\n",
    "#### 6.1 Create features and target\n",
    "##### X to be our features and y to be our target\n",
    "# Features and target\n",
    "X = train_cleaned.drop(columns=['status_group', 'status_binary'], errors='ignore')\n",
    "y = train_cleaned['status_binary']\n",
    "\n",
    "#### 6.2 Train/Test split for Validation\n",
    "##### Split data and Use a portion of the training set to validate the model\n",
    "#use test size of 0.2 and random state 42\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split( X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "### 6.3 Standardization\n",
    "##### in our dataset, several numerical feature exist on **different scales** we will standardize them using standardscaler:\n",
    "\n",
    "- `amount_tsh_log` might range between 0 and 15\n",
    "- `gps_height` can span from -50 to over 2000\n",
    "- `well_age` ranges from 0 to over 60 years\n",
    "- `population_log` values vary depending on the community size\n",
    "# Instantiate the scaler\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "### 6.4 Logistic Model\n",
    "#### will develop a logistic model to be my  first base model.\n",
    "#  Instantiate the model\n",
    "log_model = LogisticRegression(max_iter=300,solver='saga', random_state=42)\n",
    "#  Fit the model\n",
    "log_model.fit(X_train_scaled, y_train)\n",
    "#  Make predictions\n",
    "y_pred = log_model.predict(X_val_scaled)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# 4. Evaluate performance\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "#print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print(\"\\n=== Accuracy Score ===\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_val, y_pred) * 100, 2), \"%\")\n",
    "#Visualize our results\n",
    "# Get predictions and confusion matrix\n",
    "y_pred = log_model.predict(X_val_scaled)\n",
    "cm = confusion_matrix(y_val, y_pred, labels=[\"functional\", \"needs_attention\"])\n",
    "\n",
    "# Plot using seaborn heatmap\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu', \n",
    "            xticklabels=[\"functional\", \"needs_attention\"], \n",
    "            yticklabels=[\"functional\", \"needs_attention\"])\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "##### True Positives (TP): 4900 Wells that are functional and were correctly predicted as functional.\n",
    "\n",
    "#### False Negatives (FN): 837 Wells that are functional, but were predicted as needing attention.These are missed good wells ,could lead to unnecessary maintenance.\n",
    "\n",
    "#### False Positives (FP): 1532 Wells that need attention, but were predicted as functional.This is a serious concern because these wells may fail and get overlooked.\n",
    "\n",
    "#### True Negatives (TN): 3256 Wells that need attention, and were correctly predicted.\n",
    "### Visualize metrics-Accurancy,recall,precision and F1Score\n",
    "# Get classification report as dict\n",
    "report = classification_report(y_val, y_pred, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Check the columns available\n",
    "print(report_df.columns)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(report_df.loc[['functional', 'needs_attention'], ['precision', 'recall', 'f1-score']], annot=True, cmap='YlGnBu', fmt='.2f')\n",
    "plt.title('Classification Report Metrics Heatmap')\n",
    "plt.show()\n",
    "##### as per our visualiztion of metrics :\n",
    "#### Precision = 0.80 (80% of predicted wells needing attention were correct)\n",
    "\n",
    "##### Recall = 0.68 (68% of actual wells needing attention were found)\n",
    "\n",
    "##### F1-score = 0.73 (balance of the two)\n",
    "#### 6.5 Decision trees\n",
    "##### will develop decision tree model as my secod base model\n",
    "# Initialize model with some parameters to control overfitting\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_leaf=10)\n",
    "\n",
    "# Train model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_dt = dt_model.predict(X_val)\n",
    "\n",
    "# Evaluate results\n",
    "print(classification_report(y_val, y_pred_dt))\n",
    "#visualize the decision tree\n",
    "plt.figure(figsize=(20, 10))  # Adjust size as needed\n",
    "plot_tree(\n",
    "    dt_model,\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=dt_model.classes_,\n",
    "    filled=True,\n",
    "    max_depth=3,  # Limit depth shown for readability\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Decision Tree Visualization (Depth=3)\")\n",
    "plt.show()\n",
    "##### as per the decision tree model:\n",
    "##### **Functional**\n",
    "##### Recall = 0.87: The model correctly identifies 87% of truly functional wells.\n",
    "##### Precision = 0.74: Out of all predicted functional wells, 74% were actually functional.\n",
    "\n",
    "##### **Needs Attention**\n",
    "##### Recall = 0.64: The model is missing 36% of the \"needs_attention\" wells (false negatives).\n",
    "##### Precision = 0.81: When it predicts \"needs_attention\", it's right 81% of the time — pretty good.\n",
    "\n",
    "##### **Overall Accuracy: 77%**\n",
    "##### That’s strong and comparable to many real-world models, especially for imbalanced classes.\n",
    "#### 6.5 **Random forest model**\n",
    "##### We will develop Random forest model as our third base model\n",
    "# 1. Initialize model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "# 2. Train model\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 3. Predict\n",
    "y_pred_rf = rf_model.predict(X_val_scaled)\n",
    "\n",
    "# 4. Evaluate\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_val, y_pred_rf))\n",
    "\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_val, y_pred_rf))\n",
    "print(\"\\n=== Accuracy Score ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred_rf) * 100:.2f} %\")\n",
    "##### The Random Forest classifier achieved an overall accuracy of 81.87% on the validation set. For the functional class, the model reached a precision of 0.82, a recall of 0.86, and an F1-score of 0.84, based on 5,737 samples. For the needs attention class, it obtained a precision of 0.82, a recall of 0.77, and an F1-score of 0.79, evaluated on 4,788 samples.\n",
    "#### 6.6 **Comparison for the three Models**\n",
    "##### Logistic Regression, as the first base model, achieved an overall accuracy of 77.49%. It performed well on the functional class with a recall of 85%, but struggled with the needs attention class, achieving only 68% recall. This indicates that while it is effective at identifying working waterpoints, it tends to miss a significant number of those needing attention.\n",
    "\n",
    "##### Decision Tree, the second base model, had a slightly lower accuracy of 77.0%. It achieved a high recall of 87% for functional waterpoints but only 64% recall for needs attention. This suggests it overemphasized the majority class, making it prone to overfitting and less reliable for detecting waterpoints in need of repair.\n",
    "\n",
    "##### Random Forest, the third base model, outperformed both previous models with an accuracy of 81.87%. It maintained a strong balance between precision and recall across both classes achieving 86% recall for functional and 77% recall for needs attention. This makes it the most robust and reliable model among the three for classifying waterpoint functionality\n",
    "#### 7. Hyperparameter Tuning\n",
    "##### From the comparison of the three base models , Random forest stood out therefore we will tune it further by balancing the class weights between functional and needs attendion.\n",
    "\n",
    "# Initialize Random Forest with balanced class weights\n",
    "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "\n",
    "##### Both classes attain equal precision (0.82), indicating reliability in positive predictions.\n",
    "#### Recall for functional (0.86) is strong, meaning the majority of working wells are identified.\n",
    "#### Recall for needs_attention (0.77) is slightly lower, meaning 23% of failing wells are missed.\n",
    "#### The F1-scores (0.84 and 0.79) show a good balance between precision and recall.\n",
    "#### Overall, the model achieves 82% accuracy, with macro and weighted averages supporting class balance\n",
    "#### 7.1 ROC AND AUC  ,Visualization  of Random Forest model\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "# Get probability estimates for the positive class (\"needs_attention\")\n",
    "y_probs = rf_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Compute false positive rate (FPR), true positive rate (TPR), and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_probs, pos_label='needs_attention')\n",
    "\n",
    "# Compute the AUC\n",
    "roc_auc = roc_auc_score(y_val, y_probs)\n",
    "print(f\"Validation ROC AUC: {roc_auc:.3f}\")\n",
    "#Visualize\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})', lw=2)\n",
    "plt.plot([0,1], [0,1], 'k--', label='Random baseline')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve-Random Forest')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "##### Results\n",
    "##### Having a ROC of 0.84 Shows that our model is doing good being less than 0.5\n",
    "#### AUC represents the probability that your model will assign a higher “needs_attention” score to a truly failing well than to a functional one.\n",
    "\n",
    "#### At 0.814, this implies there's about an 81.4% chance a randomly sampled failing well will be ranked above a functional one well above random chance (50%)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
